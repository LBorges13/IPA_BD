{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LBorges13/IPA_BD/blob/main/12_GC_BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Técnicas de Web Scraping"
      ],
      "metadata": {
        "id": "A48Ll8gju7Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BeautifulSoup"
      ],
      "metadata": {
        "id": "IItT-Tt1v_wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bibliotecas:\n",
        "\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "VjdX4c5pwjOe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title URLs:\n",
        "\n",
        "urls = [\"https://books.toscrape.com\"]"
      ],
      "metadata": {
        "id": "J3h12hPPwmi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Textual:\n",
        "\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    print(f\"\\nTexto do HTML em: {url}\\n\")\n",
        "\n",
        "    texto = soup.get_text(separator=\"\\n\", strip=True)\n",
        "    print(texto)\n",
        "\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FcRgM7m6dDsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Links:\n",
        "\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    print(f\"\\nLinks em: {url}\")\n",
        "\n",
        "    for link in soup.find_all(\"a\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href:\n",
        "            full_url = requests.compat.urljoin(url, href)\n",
        "            html_link = f'<a href=\"{full_url}\">'\n",
        "            print(html_link)\n",
        "\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "id": "5OS023fQvrAU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Imagens:\n",
        "\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    print(f\"\\nImagens SRC em: {url}\")\n",
        "\n",
        "    for img in soup.find_all('img', src=True):\n",
        "        src = img.get('src')\n",
        "        full_src = requests.compat.urljoin(url, src)\n",
        "        html_link = f'<a href=\"{full_src}\"'\n",
        "        print(html_link)\n",
        "\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7CPpy6m9ZNEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapy"
      ],
      "metadata": {
        "id": "aXbAlH_kwDC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bibliotecas:\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin"
      ],
      "metadata": {
        "id": "tWrcPTGNwH5J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Textual:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    resp = requests.get(url)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    titulo = soup.title.string.strip() if soup.title else \"Sem título\"\n",
        "    print(f\"\\nPágina: {url}\\nTítulo: {titulo}\\n\")\n",
        "\n",
        "    texto = soup.get_text(separator=\"\\n\", strip=True)\n",
        "    print(texto)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WHNB94IDp1z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Links:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    resp = requests.get(url)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    print(url, soup.title.string.strip())\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "id": "B0r57ac0qWGL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Imagens:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    resp = requests.get(url)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    titulo = soup.title.string.strip() if soup.title else \"Sem título\"\n",
        "    print(f\"\\nPágina: {url}\\nTítulo: {titulo}\\n\")\n",
        "\n",
        "    print(f\"Imagens SRC em: {url}\")\n",
        "    for img in soup.find_all('img', src=True):\n",
        "        src = img.get('src')\n",
        "        full_src = urljoin(url, src)\n",
        "        html_link = f'<a href=\"{full_src}\">'\n",
        "        print(html_link)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F4ulXQytqlqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selenium"
      ],
      "metadata": {
        "id": "uqeJk5XlyTnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bibliotecas:\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)"
      ],
      "metadata": {
        "id": "Lr-TyKwXDbyD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Texto:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    driver.get(url)\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    titulo = soup.title.string.strip() if soup.title else \"Sem título\"\n",
        "    print(f\"\\nPágina: {url}\\nTítulo: {titulo}\\n\")\n",
        "\n",
        "    texto = soup.get_text(separator=\"\\n\", strip=True)\n",
        "    print(texto)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "id": "0RWehia8zdrG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Links:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    driver.get(url)\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    titulo = soup.title.string.strip() if soup.title else \"Sem título\"\n",
        "    print(url, titulo)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "id": "EqoYlMl2zele",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Raspagem de Imagem:\n",
        "\n",
        "url = \"https://books.toscrape.com/\"\n",
        "visited = set()\n",
        "\n",
        "def crawl(url):\n",
        "    if url in visited:\n",
        "        return\n",
        "    visited.add(url)\n",
        "\n",
        "    driver.get(url)\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    titulo = soup.title.string.strip() if soup.title else \"Sem título\"\n",
        "    print(f\"\\nPágina: {url}\\nTítulo: {titulo}\\n\")\n",
        "\n",
        "    print(f\"Imagens SRC em: {url}\")\n",
        "    for img in soup.find_all('img', src=True):\n",
        "        src = img.get('src')\n",
        "        full_src = urljoin(url, src)\n",
        "        html_link = f'<a href=\"{full_src}\">'\n",
        "        print(html_link)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        next_url = urljoin(url, a[\"href\"])\n",
        "        if next_url.startswith(\"https://books.toscrape.com/\"):\n",
        "            crawl(next_url)\n",
        "\n",
        "crawl(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5S2BKm5kyOxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.quit()"
      ],
      "metadata": {
        "id": "DfbxJ5CXqm1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}